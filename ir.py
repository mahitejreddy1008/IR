# -*- coding: utf-8 -*-
"""IR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zcfagXNfi5If5VvuL7YE-pva-hipvasp
"""

import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

import pandas as pd

# Load the CSV file using pandas
df = pd.read_csv('your_file.csv')

# Assuming the document text is in a column named 'document'
documents = df['document'].tolist()  # Replace 'document' with the actual column name

def preprocess(text):
  text = re.sub('[^A-Za-z0-9]+   ', '', text)
  text = text.lower()
  text = text.replace("\n"," ")
  text = text.replace("\ufeff","")
  return text

preprocessed_docs = [preprocess(doc) for doc in documents]

def tokenize(text):
    return text.lower().split()

# Tokenize all documents
tokenized_docs = [tokenize(doc) for doc in documents]

word_count = {}

# Iterate over each document
for doc in tokenized_docs:
    for word in doc:
        if word in word_count:
            word_count[word] += 1
        else:
            word_count[word] = 1

threshold = 10  # Adjust this based on your dataset

# Create a set of stopwords based on the threshold
stopwords = {word for word, count in word_count.items() if count > threshold}

# Filter the documents by removing stopwords
filtered_docs = [[word for word in doc if word not in stopwords] for doc in tokenized_docs]

# Initialize an empty dictionary to store the inverted index
inverted_index = {}

# Build the inverted index by iterating over each document
for doc_id, doc in enumerate(filtered_docs):
    for word in doc:
        if word in inverted_index:
            inverted_index[word].add(doc_id)  # Add the document ID to the set
        else:
            inverted_index[word] = {doc_id}  # Initialize with a set containing the document ID

def boolean_query(query, inverted_index):
    query = query.lower().split()
    result_set = set(range(len(documents)))  # Start with all documents

    i = 0
    while i < len(query):
        token = query[i]
        if token == "and":
            i += 1
            continue
        elif token == "not":
            i += 1
            word = query[i]
            if word in inverted_index:
                result_set -= inverted_index[word]
            else:
                result_set -= set()
        elif token == "or":
            i += 1
            word = query[i]
            if word in inverted_index:
                result_set |= inverted_index[word]
        else:
            if token in inverted_index:
                result_set &= inverted_index[token]
            else:
                result_set &= set()

        i += 1

    return result_set

# Example usage
result_docs = boolean_query("word1 AND word2 NOT word3", inverted_index)
print(f"Documents matching the query: {result_docs}")

def cosine_distance(v1,v2):
  return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))

def retriever(content,query,top_k):
  result = {}
  for i in range(len(content)):
    result[i] = cosine_distance(content[i],query)

  result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1],reverse=True)}
  return list(result.keys())[:top_k]

vectorizer = TfidfVectorizer()
tfidf_para = vectorizer.fit_transform(preprocessed_paragraphs).toarray()

query = ["international machine learning conference".lower()]
tfidf_query = list(vectorizer.transform(query).toarray()[0])
top_k = 5

results = retriever(tfidf_para,tfidf_query,top_k)
results

for i in range(len(results)):
  print("TOP ",i+1,":",paragraphs[results[i]])