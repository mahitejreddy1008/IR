{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BoAE4BKUSKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab81567-2228-4d7f-f6b0-9ab5cdb6146e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Query Vector (TF-IDF values):\n",
            "[[ 0.18555356 -0.04917121  1.00594508  0.64124571  0.36000696  0.2162305\n",
            "  -0.07934205 -0.04917121  0.37839113]]\n",
            "Rank 1: Document 5 (Score: 1.0295) - 'the fox and the cat'\n",
            "Rank 2: Document 3 (Score: 0.9043) - 'the cat and the hat'\n",
            "Rank 3: Document 1 (Score: 0.8886) - 'the cat in the hat'\n",
            "Rank 4: Document 4 (Score: 0.3256) - 'the quick red fox'\n",
            "Rank 5: Document 2 (Score: 0.3256) - 'the quick brown fox'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Example documents and query\n",
        "documents = [\n",
        "    \"the cat in the hat\",\n",
        "    \"the quick brown fox\",\n",
        "    \"the cat and the hat\",\n",
        "    \"the quick red fox\",\n",
        "    \"the fox and the cat\"\n",
        "]\n",
        "\n",
        "query = \"cat fox\"\n",
        "\n",
        "# Relevance feedback\n",
        "relevant_docs_indices = [0, 2]  # indices of relevant documents\n",
        "non_relevant_docs_indices = [1, 3]  # indices of non-relevant documents\n",
        "\n",
        "# Parameters for Rocchio algorithm\n",
        "alpha = 1.0\n",
        "beta = 0.75\n",
        "gamma = 0.15\n",
        "\n",
        "# Preprocessing: TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "query_vector = vectorizer.transform([query])\n",
        "\n",
        "# Rocchio Algorithm\n",
        "def rocchio(query_vector, tfidf_matrix, relevant_indices, non_relevant_indices, alpha, beta, gamma):\n",
        "    # Compute the centroids of relevant and non-relevant documents\n",
        "    relevant_docs = tfidf_matrix[relevant_indices]\n",
        "    non_relevant_docs = tfidf_matrix[non_relevant_indices]\n",
        "\n",
        "    # Average relevant and non-relevant document vectors\n",
        "    if len(relevant_indices) > 0:\n",
        "        relevant_centroid = relevant_docs.mean(axis=0)\n",
        "    else:\n",
        "        relevant_centroid = np.zeros(query_vector.shape)\n",
        "\n",
        "    if len(non_relevant_indices) > 0:\n",
        "        non_relevant_centroid = non_relevant_docs.mean(axis=0)\n",
        "    else:\n",
        "        non_relevant_centroid = np.zeros(query_vector.shape)\n",
        "\n",
        "    # Rocchio update formula\n",
        "    new_query_vector = alpha * query_vector + beta * relevant_centroid - gamma * non_relevant_centroid\n",
        "\n",
        "    return new_query_vector\n",
        "\n",
        "# Compute the new query vector using Rocchio algorithm\n",
        "new_query_vector = rocchio(query_vector, tfidf_matrix, relevant_docs_indices, non_relevant_docs_indices, alpha, beta, gamma)\n",
        "\n",
        "# Print the updated query vector\n",
        "print(\"Updated Query Vector (TF-IDF values):\")\n",
        "print(new_query_vector)\n",
        "# newly added line\n",
        "new_query_vector = csr_matrix(new_query_vector)\n",
        "\n",
        "# Ranking documents based on new query vector\n",
        "def rank_documents(new_query_vector, tfidf_matrix):\n",
        "    # Compute cosine similarity between new query vector and document vectors\n",
        "    scores = (tfidf_matrix * new_query_vector.T).toarray().flatten()\n",
        "\n",
        "    # Rank documents based on scores\n",
        "    ranked_docs = np.argsort(scores)[::-1]\n",
        "\n",
        "    for i, doc_index in enumerate(ranked_docs):\n",
        "        print(f\"Rank {i+1}: Document {doc_index+1} (Score: {scores[doc_index]:.4f}) - '{documents[doc_index]}'\")\n",
        "\n",
        "# Rank documents using the updated query vector\n",
        "rank_documents(new_query_vector, tfidf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIN"
      ],
      "metadata": {
        "id": "41_6mJJBHQi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example documents and query\n",
        "documents = [\n",
        "    \"the cat in the hat\",\n",
        "    \"the quick brown fox\",\n",
        "    \"the cat and the hat\",\n",
        "    \"the quick red fox\",\n",
        "    \"the fox and the cat\"\n",
        "]\n",
        "\n",
        "query = \"cat hat\"\n",
        "\n",
        "# Preprocessing: Tokenize documents and query\n",
        "def tokenize(doc):\n",
        "    return doc.lower().split()\n",
        "\n",
        "doc_tokens = [set(tokenize(doc)) for doc in documents]\n",
        "query_tokens = set(tokenize(query))\n",
        "\n",
        "# Inverse document frequency calculation\n",
        "def compute_idf(doc_tokens, num_docs):\n",
        "    term_doc_count = {}\n",
        "    for tokens in doc_tokens:\n",
        "        for token in tokens:\n",
        "            if token in term_doc_count:\n",
        "                term_doc_count[token] += 1\n",
        "            else:\n",
        "                term_doc_count[token] = 1\n",
        "\n",
        "    idf = {}\n",
        "    for term, count in term_doc_count.items():\n",
        "        idf[term] = np.log((num_docs - count + 0.5) / (count + 0.5))\n",
        "\n",
        "    return idf\n",
        "\n",
        "# Binary Independence Model\n",
        "def compute_bim_score(doc_tokens, query_tokens, idf, num_docs):\n",
        "    scores = []\n",
        "\n",
        "    for tokens in doc_tokens:\n",
        "        score = 0\n",
        "        for term in query_tokens:\n",
        "            if term in idf:\n",
        "                if term in tokens:  # Term is present in document\n",
        "                    score += idf[term]\n",
        "                else:  # Term is not present in document\n",
        "                    score += np.log((0.5) / (num_docs + 0.5))\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Main function to compute and rank documents\n",
        "def rank_documents(documents, query):\n",
        "    num_docs = len(documents)\n",
        "    idf = compute_idf(doc_tokens, num_docs)\n",
        "    scores = compute_bim_score(doc_tokens, query_tokens, idf, num_docs)\n",
        "\n",
        "    print(scores)\n",
        "\n",
        "    ranked_docs = np.argsort(scores)[::-1]\n",
        "    print(ranked_docs)\n",
        "\n",
        "    for i, doc_index in enumerate(ranked_docs):\n",
        "        print(f\"Rank {i+1}: Document {doc_index+1} (Score: {scores[doc_index]:.4f}) - '{documents[doc_index]}'\")\n",
        "\n",
        "# Rank documents based on query\n",
        "rank_documents(documents, query)\n"
      ],
      "metadata": {
        "id": "2R3pYf8sUdnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f3ecfb-dc84-4122-fd52-bb2d6a906a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, -4.795790545596741, 0.0, -4.795790545596741, -2.7343675094195836]\n",
            "[2 0 4 3 1]\n",
            "Rank 1: Document 3 (Score: 0.0000) - 'the cat and the hat'\n",
            "Rank 2: Document 1 (Score: 0.0000) - 'the cat in the hat'\n",
            "Rank 3: Document 5 (Score: -2.7344) - 'the fox and the cat'\n",
            "Rank 4: Document 4 (Score: -4.7958) - 'the quick red fox'\n",
            "Rank 5: Document 2 (Score: -4.7958) - 'the quick brown fox'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "U2VEJ8IyWyyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file using pandas\n",
        "df = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Assuming the document text is in a column named 'document'\n",
        "documents = df['document'].tolist()  # Replace 'document' with the actual column name\n",
        "\n",
        "def preprocess(text):\n",
        "  text = re.sub('[^A-Za-z0-9]+   ', '', text)\n",
        "  text = text.lower()\n",
        "  text = text.replace(\"\\n\",\" \")\n",
        "  text = text.replace(\"\\ufeff\",\"\")\n",
        "  return text\n",
        "\n",
        "preprocessed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Tokenize all documents\n",
        "tokenized_docs = [tokenize(doc) for doc in documents]\n",
        "\n",
        "word_count = {}\n",
        "\n",
        "# Iterate over each document\n",
        "for doc in tokenized_docs:\n",
        "    for word in doc:\n",
        "        if word in word_count:\n",
        "            word_count[word] += 1\n",
        "        else:\n",
        "            word_count[word] = 1\n",
        "\n",
        "threshold = 10  # Adjust this based on your dataset\n",
        "\n",
        "# Create a set of stopwords based on the threshold\n",
        "stopwords = {word for word, count in word_count.items() if count > threshold}\n",
        "\n",
        "# Filter the documents by removing stopwords\n",
        "filtered_docs = [[word for word in doc if word not in stopwords] for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "ic6dFYZMWxll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "eUwdwTFQYOf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
        "\n",
        "# Sample relevance judgments (ground truth)\n",
        "# 1 = relevant, 0 = non-relevant\n",
        "true_relevance = [1, 0, 1, 1, 0]  # This is the ground truth (relevant or not)\n",
        "# binary_list = [1] * count_of_ones + [0] * (total_length - count_of_ones)\n",
        "\n",
        "# # Shuffle the list to randomize the position of 1's and 0's\n",
        "# random.shuffle(binary_list)\n",
        "\n",
        "# Assume these are the BM25 scores we computed previously\n",
        "bm25_scores = [2.2273, 2.0479, 1.9872, 1.7328, 1.4151]\n",
        "\n",
        "# Binary prediction based on a threshold (e.g., 1.5 for BM25)\n",
        "threshold = 1.5\n",
        "predicted_relevance = [1 if score > threshold else 0 for score in bm25_scores]  # Binary classification\n",
        "\n",
        "# Precision\n",
        "precision_val = precision_score(true_relevance, predicted_relevance)\n",
        "\n",
        "# Recall\n",
        "recall_val = recall_score(true_relevance, predicted_relevance)\n",
        "\n",
        "# F1-Score\n",
        "f1_val = f1_score(true_relevance, predicted_relevance)\n",
        "\n",
        "# Mean Average Precision (MAP) using raw BM25 scores and true relevance labels\n",
        "map_val = average_precision_score(true_relevance, bm25_scores)\n",
        "\n",
        "# Output the metrics\n",
        "print(f\"Precision: {precision_val:.4f}\")\n",
        "print(f\"Recall: {recall_val:.4f}\")\n",
        "print(f\"F1-Score: {f1_val:.4f}\")\n",
        "print(f\"Mean Average Precision (MAP): {map_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "2RHAsoijW9CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BM25 - Same as BIN"
      ],
      "metadata": {
        "id": "Twji4a7phlJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example documents and query\n",
        "documents = [\n",
        "    \"the cat in the hat\",\n",
        "    \"the quick brown fox\",\n",
        "    \"the cat and the hat\",\n",
        "    \"the quick red fox\",\n",
        "    \"the fox and the cat\"\n",
        "]\n",
        "\n",
        "query = \"cat fox\"\n",
        "\n",
        "# Preprocessing: Tokenize documents and query\n",
        "def tokenize(doc):\n",
        "    return doc.lower().split()\n",
        "\n",
        "doc_tokens = [tokenize(doc) for doc in documents]  # Modified from set(tokenize(doc)) to just tokenize(doc) to allow term frequency count\n",
        "query_tokens = tokenize(query)\n",
        "\n",
        "# BM25 Parameters (NEW PARAMETERS)\n",
        "k1 = 1.5  # term frequency saturation, typically between 1.2 and 2\n",
        "b = 0.75  # length normalization, typically 0.75\n",
        "\n",
        "# Inverse document frequency calculation for BM25 (MODIFIED)\n",
        "def compute_idf(doc_tokens, num_docs):\n",
        "    term_doc_count = {}\n",
        "    for tokens in doc_tokens:\n",
        "        for token in set(tokens):  # Only count a term once per document\n",
        "            if token in term_doc_count:\n",
        "                term_doc_count[token] += 1\n",
        "            else:\n",
        "                term_doc_count[token] = 1\n",
        "\n",
        "    idf = {}\n",
        "    for term, count in term_doc_count.items():\n",
        "        # BM25 IDF formula (MODIFIED)\n",
        "        idf[term] = np.log((num_docs - count + 0.5) / (count + 0.5) + 1)\n",
        "\n",
        "    return idf\n",
        "\n",
        "# Compute BM25 score (MODIFIED)\n",
        "def compute_bm25_score(doc_tokens, query_tokens, idf, num_docs):\n",
        "    scores = []\n",
        "    avg_doc_length = np.mean([len(tokens) for tokens in doc_tokens])  # Compute average document length (NEW)\n",
        "\n",
        "    for tokens in doc_tokens:\n",
        "        score = 0\n",
        "        doc_len = len(tokens)  # Get document length (NEW)\n",
        "        token_counts = {token: tokens.count(token) for token in tokens}  # Term frequency calculation (NEW)\n",
        "\n",
        "        for term in query_tokens:\n",
        "            if term in idf:\n",
        "                # Calculate term frequency (TF) (NEW)\n",
        "                tf = token_counts.get(term, 0)\n",
        "                # BM25 formula (MODIFIED)\n",
        "                term_score = idf[term] * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_doc_length))))\n",
        "                score += term_score\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Main function to compute and rank documents\n",
        "def rank_documents(documents, query):\n",
        "    num_docs = len(documents)\n",
        "    idf = compute_idf(doc_tokens, num_docs)  # Uses BM25 IDF (MODIFIED)\n",
        "    scores = compute_bm25_score(doc_tokens, query_tokens, idf, num_docs)  # Uses BM25 score calculation (MODIFIED)\n",
        "\n",
        "    ranked_docs = np.argsort(scores)[::-1]  # Same sorting mechanism as before\n",
        "\n",
        "    for i, doc_index in enumerate(ranked_docs):\n",
        "        print(f\"Rank {i+1}: Document {doc_index+1} (Score: {scores[doc_index]:.4f}) - '{documents[doc_index]}'\")\n",
        "\n",
        "# Rank documents based on query using BM25 (MODIFIED)\n",
        "rank_documents(documents, query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F92VwB-vg54K",
        "outputId": "7a176208-dafc-447d-d8fd-6e5a711e6370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1: Document 5 (Score: 1.0374) - 'the fox and the cat'\n",
            "Rank 2: Document 4 (Score: 0.5726) - 'the quick red fox'\n",
            "Rank 3: Document 2 (Score: 0.5726) - 'the quick brown fox'\n",
            "Rank 4: Document 3 (Score: 0.5187) - 'the cat and the hat'\n",
            "Rank 5: Document 1 (Score: 0.5187) - 'the cat in the hat'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_AN_Nsxg6sI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}